{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOGc5TgDiGPuckJ0z0r2b2R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aynahd/VeriAssist/blob/main/emotionsTry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jUF7gHlyLOtN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9X_WOMgK3bv",
        "outputId": "6f5c9063-1869-4104-8af1-4aa84879171e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             id                                               text  \\\n",
            "0       eew5j0j                                    That game hurt.   \n",
            "1       eemcysk   >sexuality shouldn’t be a grouping category I...   \n",
            "2       ed2mah1     You do right, if you don't care then fuck 'em!   \n",
            "3       eeibobj                                 Man I love reddit.   \n",
            "4       eda6yn6  [NAME] was nowhere near them, he was by the Fa...   \n",
            "...         ...                                                ...   \n",
            "140747  edi4nbj  Because Rian is a hack...and TLJ sucks. That's...   \n",
            "140748  efbxvgo       The tree actually moved!!! Who is this guy?!   \n",
            "140749  edodrci  Wait his nostrils are plugged? Isn’t that the ...   \n",
            "140750  ed387lu  My great aunt was saved by the serum delivered...   \n",
            "140751  edkr8ou  Nothing wrong with striving. Forcibly making p...   \n",
            "\n",
            "       example_very_unclear  admiration  amusement  anger  annoyance  \\\n",
            "0                     False         0.0        0.0    0.0        0.0   \n",
            "1                      True         0.0        0.0    0.0        0.0   \n",
            "2                     False         0.0        0.0    0.0        0.0   \n",
            "3                     False         0.0        0.0    0.0        0.0   \n",
            "4                     False         0.0        0.0    0.0        0.0   \n",
            "...                     ...         ...        ...    ...        ...   \n",
            "140747                False         0.0        1.0    0.0        0.0   \n",
            "140748                False         0.0        0.0    0.0        0.0   \n",
            "140749                False         0.0        0.0    0.0        0.0   \n",
            "140750                False         0.0        0.0    0.0        0.0   \n",
            "140751                  NaN         NaN        NaN    NaN        NaN   \n",
            "\n",
            "        approval  caring  confusion  ...  love  nervousness  optimism  pride  \\\n",
            "0            0.0     0.0        0.0  ...   0.0          0.0       0.0    0.0   \n",
            "1            0.0     0.0        0.0  ...   0.0          0.0       0.0    0.0   \n",
            "2            0.0     0.0        0.0  ...   0.0          0.0       0.0    0.0   \n",
            "3            0.0     0.0        0.0  ...   1.0          0.0       0.0    0.0   \n",
            "4            0.0     0.0        0.0  ...   0.0          0.0       0.0    0.0   \n",
            "...          ...     ...        ...  ...   ...          ...       ...    ...   \n",
            "140747       0.0     0.0        0.0  ...   0.0          0.0       0.0    0.0   \n",
            "140748       0.0     0.0        0.0  ...   0.0          0.0       0.0    0.0   \n",
            "140749       0.0     0.0        0.0  ...   0.0          0.0       0.0    0.0   \n",
            "140750       0.0     0.0        0.0  ...   0.0          0.0       0.0    0.0   \n",
            "140751       NaN     NaN        NaN  ...   NaN          NaN       NaN    NaN   \n",
            "\n",
            "        realization  relief  remorse  sadness  surprise  neutral  \n",
            "0               0.0     0.0      0.0      1.0       0.0      0.0  \n",
            "1               0.0     0.0      0.0      0.0       0.0      0.0  \n",
            "2               0.0     0.0      0.0      0.0       0.0      1.0  \n",
            "3               0.0     0.0      0.0      0.0       0.0      0.0  \n",
            "4               0.0     0.0      0.0      0.0       0.0      1.0  \n",
            "...             ...     ...      ...      ...       ...      ...  \n",
            "140747          0.0     0.0      0.0      0.0       0.0      0.0  \n",
            "140748          0.0     0.0      0.0      0.0       1.0      0.0  \n",
            "140749          0.0     0.0      0.0      0.0       0.0      1.0  \n",
            "140750          0.0     0.0      0.0      0.0       0.0      1.0  \n",
            "140751          NaN     NaN      NaN      NaN       NaN      NaN  \n",
            "\n",
            "[140752 rows x 31 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-8772a0132bf0>:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('emotions.csv')\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('emotions.csv')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tag import pos_tag\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def process(text_series):\n",
        "\n",
        "    text_series = text_series.str.lower()\n",
        "\n",
        "    def process(text):\n",
        "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        stop_words = {\"in\", \"on\", \"the\", \"a\", \"an\", \"at\", \"with\", \"my\"}\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        tagged = pos_tag(tokens)\n",
        "        filtered = [word for word, tag in tagged if tag.startswith(('NN', 'JJ', 'VB'))]\n",
        "\n",
        "        tokens = [stemmer.stem(word) for word in filtered]\n",
        "        return tokens\n",
        "\n",
        "    return text_series.apply(process)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKwT7SCzM2nn",
        "outputId": "059a63a8-f8ff-446a-a123-cb86a6ff7a82"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = df['text'].iloc[:5]\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwLf89WPPVyO",
        "outputId": "d9b21ffa-8862-40ff-e1b9-57aa67e030d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                      That game hurt.\n",
            "1     >sexuality shouldn’t be a grouping category I...\n",
            "2       You do right, if you don't care then fuck 'em!\n",
            "3                                   Man I love reddit.\n",
            "4    [NAME] was nowhere near them, he was by the Fa...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = process(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bAuyHLvPZ5E",
        "outputId": "2aab3d59-9661-4395-daf8-9bf10cef5c80"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                         [game, hurt]\n",
            "1    [sexual, shouldnt, be, group, categori, make, ...\n",
            "2                                 [do, dont, care, em]\n",
            "3                               [man, i, love, reddit]\n",
            "4                               [name, wa, wa, falcon]\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def ner_anonymizer(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    anonymized_text = text\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in {\"PERSON\", \"GPE\", \"DATE\", \"ORG\"}:\n",
        "            anonymized_text = anonymized_text.replace(ent.text, f\"<{ent.label_}>\")\n",
        "\n",
        "    return anonymized_text\n"
      ],
      "metadata": {
        "id": "5HmexZVrYpKn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "emotion_columns = [\n",
        "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
        "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
        "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
        "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
        "    'remorse', 'sadness', 'surprise', 'neutral'\n",
        "]\n",
        "df['emotion'] = df[emotion_columns].idxmax(axis=1)\n",
        "\n",
        "df2 = df.drop(columns=emotion_columns)\n",
        "\n",
        "print(df2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfJjLFCMaNo0",
        "outputId": "efa3e449-f00e-4f5e-84ed-0f0ce4841cc6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             id                                               text  \\\n",
            "0       eew5j0j                                    That game hurt.   \n",
            "1       eemcysk   >sexuality shouldn’t be a grouping category I...   \n",
            "2       ed2mah1     You do right, if you don't care then fuck 'em!   \n",
            "3       eeibobj                                 Man I love reddit.   \n",
            "4       eda6yn6  [NAME] was nowhere near them, he was by the Fa...   \n",
            "...         ...                                                ...   \n",
            "140747  edi4nbj  Because Rian is a hack...and TLJ sucks. That's...   \n",
            "140748  efbxvgo       The tree actually moved!!! Who is this guy?!   \n",
            "140749  edodrci  Wait his nostrils are plugged? Isn’t that the ...   \n",
            "140750  ed387lu  My great aunt was saved by the serum delivered...   \n",
            "140751  edkr8ou  Nothing wrong with striving. Forcibly making p...   \n",
            "\n",
            "       example_very_unclear     emotion  \n",
            "0                     False     sadness  \n",
            "1                      True  admiration  \n",
            "2                     False     neutral  \n",
            "3                     False        love  \n",
            "4                     False     neutral  \n",
            "...                     ...         ...  \n",
            "140747                False   amusement  \n",
            "140748                False    surprise  \n",
            "140749                False     neutral  \n",
            "140750                False     neutral  \n",
            "140751                  NaN         NaN  \n",
            "\n",
            "[140752 rows x 4 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-32fd403a09bf>:10: FutureWarning: The behavior of DataFrame.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
            "  df['emotion'] = df[emotion_columns].idxmax(axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df2[df2['example_very_unclear'] == False].drop(columns=['example_very_unclear'])\n",
        "print(df3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEr0jTRRbhLE",
        "outputId": "b3944ef4-2528-4d8b-8473-1ac7f0916446"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             id                                               text     emotion\n",
            "0       eew5j0j                                    That game hurt.     sadness\n",
            "2       ed2mah1     You do right, if you don't care then fuck 'em!     neutral\n",
            "3       eeibobj                                 Man I love reddit.        love\n",
            "4       eda6yn6  [NAME] was nowhere near them, he was by the Fa...     neutral\n",
            "5       eespn2i  Right? Considering it’s such an important docu...   gratitude\n",
            "...         ...                                                ...         ...\n",
            "140746  eev3dba  And yet the guy proclaiming he understands it ...  admiration\n",
            "140747  edi4nbj  Because Rian is a hack...and TLJ sucks. That's...   amusement\n",
            "140748  efbxvgo       The tree actually moved!!! Who is this guy?!    surprise\n",
            "140749  edodrci  Wait his nostrils are plugged? Isn’t that the ...     neutral\n",
            "140750  ed387lu  My great aunt was saved by the serum delivered...     neutral\n",
            "\n",
            "[138516 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df3['emotion_encoded'] = label_encoder.fit_transform(df3['emotion'])\n",
        "print(df3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qeiyNY9b8IZ",
        "outputId": "6c7724a2-1232-416a-aa5b-f62bf8283981"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             id                                               text  \\\n",
            "0       eew5j0j                                    That game hurt.   \n",
            "2       ed2mah1     You do right, if you don't care then fuck 'em!   \n",
            "3       eeibobj                                 Man I love reddit.   \n",
            "4       eda6yn6  [NAME] was nowhere near them, he was by the Fa...   \n",
            "5       eespn2i  Right? Considering it’s such an important docu...   \n",
            "...         ...                                                ...   \n",
            "140746  eev3dba  And yet the guy proclaiming he understands it ...   \n",
            "140747  edi4nbj  Because Rian is a hack...and TLJ sucks. That's...   \n",
            "140748  efbxvgo       The tree actually moved!!! Who is this guy?!   \n",
            "140749  edodrci  Wait his nostrils are plugged? Isn’t that the ...   \n",
            "140750  ed387lu  My great aunt was saved by the serum delivered...   \n",
            "\n",
            "           emotion  emotion_encoded  \n",
            "0          sadness               26  \n",
            "2          neutral               20  \n",
            "3             love               18  \n",
            "4          neutral               20  \n",
            "5        gratitude               15  \n",
            "...            ...              ...  \n",
            "140746  admiration                0  \n",
            "140747   amusement                1  \n",
            "140748    surprise               27  \n",
            "140749     neutral               20  \n",
            "140750     neutral               20  \n",
            "\n",
            "[138516 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "uhy19O-nebCO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "df3['tokens'] = df3['text'].apply(word_tokenize)\n"
      ],
      "metadata": {
        "id": "TZmRF9CTjUts"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "word2vec_model = Word2Vec(sentences=df3['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "word2vec_model.save(\"word2vec.model\")\n"
      ],
      "metadata": {
        "id": "IaaoI0trgn7A"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def sentence_to_vector(tokens, model):\n",
        "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "df3['sentence_vector'] = df3['tokens'].apply(lambda tokens: sentence_to_vector(tokens, word2vec_model))\n",
        "\n",
        "X = np.vstack(df3['sentence_vector'].values)\n",
        "y = df3['emotion_encoded']\n"
      ],
      "metadata": {
        "id": "3jZuXqkRgx7p"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWQp4jzngXvR",
        "outputId": "76dfccd1-efc9-407d-b065-ad6d2b224bc1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.33973433439214556\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.44      0.44      2301\n",
            "           1       0.45      0.40      0.42      1217\n",
            "           2       0.30      0.26      0.28      1053\n",
            "           3       0.18      0.16      0.17      1541\n",
            "           4       0.21      0.17      0.19      2065\n",
            "           5       0.23      0.18      0.20       719\n",
            "           6       0.26      0.21      0.24       900\n",
            "           7       0.34      0.29      0.31      1029\n",
            "           8       0.19      0.18      0.18       376\n",
            "           9       0.15      0.12      0.14       923\n",
            "          10       0.22      0.18      0.20      1143\n",
            "          11       0.18      0.13      0.15       445\n",
            "          12       0.18      0.13      0.15       232\n",
            "          13       0.25      0.19      0.21       621\n",
            "          14       0.30      0.26      0.28       327\n",
            "          15       0.56      0.56      0.56      1085\n",
            "          16       0.14      0.14      0.14        70\n",
            "          17       0.22      0.18      0.20       657\n",
            "          18       0.46      0.44      0.45       705\n",
            "          19       0.13      0.07      0.09       136\n",
            "          20       0.41      0.57      0.48      7511\n",
            "          21       0.24      0.20      0.22       646\n",
            "          22       0.04      0.03      0.04        95\n",
            "          23       0.10      0.08      0.09       644\n",
            "          24       0.09      0.05      0.07       112\n",
            "          25       0.23      0.23      0.23       193\n",
            "          26       0.22      0.20      0.21       500\n",
            "          27       0.26      0.22      0.24       458\n",
            "\n",
            "    accuracy                           0.34     27704\n",
            "   macro avg       0.25      0.22      0.23     27704\n",
            "weighted avg       0.32      0.34      0.33     27704\n",
            "\n"
          ]
        }
      ]
    }
  ]
}